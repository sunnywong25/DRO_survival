{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72344cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from performance_measures import c_index, brier_score, weighted_c_index, weighted_brier_score,log_partial_lik\n",
    "from neural_models import negLogLikelihood, linearCoxPH_Regression, MLP, negLogLikelihood_per_sample\n",
    "from fairness_measures import individual_fairness, group_fairness, intersect_fairness, individual_fairness_scale, CI, C_index_difference\n",
    "\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sksurv.metrics import concordance_index_censored, brier_score, integrated_brier_score\n",
    "from sksurv.metrics import concordance_index_ipcw,cumulative_dynamic_auc\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sksurv.util import Surv\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "from scipy import optimize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fe5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'dataset': 'FLC', \n",
    "        'model': 'Linear',\n",
    "        'gpuid': '0',\n",
    "        'epochs': 500,\n",
    "        'lr': 0.01,\n",
    "        'with_scale': True,\n",
    "        'eps': 0.1,\n",
    "        'seed': 7,\n",
    "        'protect_index': 0,\n",
    "        'train_or_evaluation': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef8c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compute_survival_function import predict_survival_function  \n",
    "\n",
    "#The function below ensures that we seed all random generators with the same value to get reproducible results\n",
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "\n",
    "RANDOM_STATE = 1\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "def threshplus(x):\n",
    "    y = x.copy()\n",
    "    y[y<0]=0\n",
    "    return y\n",
    "\n",
    "def threshplus_tensor(x):\n",
    "    y = x.clone()\n",
    "    y[y<0]=0\n",
    "    return y\n",
    "\n",
    "def loss_map_chi_factory(loss_values, eps):\n",
    "    # return lambda x: np.sqrt(2)*(1.0/eps-1.0)*np.sqrt(np.mean(threshplus(loss_values-x)**2.0)) + x\n",
    "    return lambda x: np.sqrt(2 * ((1.0 / eps - 1.0)** 2.0)+1) * np.sqrt(np.mean(threshplus(loss_values - x) ** 2.0)) + x\n",
    "\n",
    "def loss_map_chi_factory_tensor(loss_values, eps, opt_eta):\n",
    "    # return np.sqrt(2)*(1.0/eps-1.0)*torch.sqrt(torch.mean(threshplus_tensor(loss_values-opt_eta)**2.0)) + opt_eta\n",
    "    return np.sqrt(2 * ((1.0 / eps - 1.0)** 2.0)+1)*torch.sqrt(torch.mean(threshplus_tensor(loss_values-opt_eta)**2.0)) + opt_eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63aa52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLC data:\n",
    "from utilities import prepare_data\n",
    "from utilities import check_arrays_survival\n",
    "from flc_data_preprocess import flc_preprocess\n",
    "from support_data_preprocess import support_preprocess\n",
    "from compas_data_preprocess import compas_preprocess\n",
    "from seer_data_preprocess import seer_preprocess\n",
    "\n",
    "#Survival Data\n",
    "if args['dataset'] == 'FLC':\n",
    "    data_x, data_y, protect_attr = flc_preprocess()\n",
    "elif args['dataset'] == 'SUPPORT':\n",
    "    data_x, data_y, protect_attr = support_preprocess()\n",
    "elif args['dataset'] == 'COMPAS':\n",
    "    data_x, data_y, protect_attr = compas_preprocess()\n",
    "elif args['dataset'] == 'SEER':\n",
    "    data_x, data_y, protect_attr = seer_preprocess()\n",
    "else:\n",
    "    print('unknown')\n",
    "\n",
    "# train-test split\n",
    "data_X_train, data_X_test, data_y_train, data_y_test, S_train, S_test = train_test_split(data_x, data_y, protect_attr, test_size=0.2,stratify=data_y[\"death\"], random_state=args['seed'])\n",
    "data_X_train, data_X_dev, data_y_train, data_y_dev, S_train, S_dev = train_test_split(data_X_train, data_y_train, S_train, test_size=0.2,stratify=data_y_train[\"death\"], random_state=args['seed'])\n",
    "#\n",
    "data_X_train, data_event_train, data_time_train = check_arrays_survival(data_X_train, data_y_train)\n",
    "data_X_train, data_event_train, data_time_train, S_train = prepare_data(data_X_train, data_event_train, data_time_train, S_train)\n",
    "\n",
    "if args['train_or_evaluation']==0:\n",
    "    data_X_test, data_event_test, data_time_test = check_arrays_survival(data_X_dev, data_y_dev)\n",
    "    data_X_test, data_event_test, data_time_test, S_test = prepare_data(data_X_test, data_event_test, data_time_test, S_dev)\n",
    "    data_y_test = data_y_dev\n",
    "else:\n",
    "    data_X_test, data_event_test, data_time_test = check_arrays_survival(data_X_test, data_y_test)\n",
    "    data_X_test, data_event_test, data_time_test, S_test = prepare_data(data_X_test, data_event_test, data_time_test, S_test)\n",
    "#\n",
    "intersectionalGroups = np.unique(S_train,axis=0) # all intersecting groups, i.e. black-women, white-man etc \n",
    "# data normalization: mean subtraction method to compute euclidean distance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_X_train)\n",
    "data_X_train = scaler.transform(data_X_train)\n",
    "data_X_test = scaler.transform(data_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7926126",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal, uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "set_random_seed()\n",
    "def simulate_dgp_1(sim_n=200, sim_tau=0.5, M = 14.0):\n",
    "    sim_X = uniform(low=0.0, high=1.0, size=(sim_n, 1))\n",
    "    sim_beta = np.array([3, 5])\n",
    "    sim_eps = normal(loc=0.0, scale=1.0, size=sim_n) - norm.ppf(sim_tau) # error term with tau-th quantile = 0\n",
    "    sim_failure_time = sim_beta[0] + sim_X @ sim_beta.reshape(-1,1)[1:, 0] + sim_eps\n",
    "    sim_censor_time = uniform(low=0.0, high=M, size=sim_n) # uniform censoring\n",
    "    sim_event = (sim_failure_time <= sim_censor_time) # event indicator\n",
    "    sim_time = sim_failure_time*sim_event + sim_censor_time*(~sim_event) # observed time\n",
    "    return sim_X, sim_failure_time, sim_censor_time, sim_event, sim_time\n",
    "\n",
    "class Local_KM:\n",
    "    def __init__(self, X, event, time, kernel, bw, is_minmax_normalize = True):\n",
    "        if is_minmax_normalize:\n",
    "            self.scaler = MinMaxScaler()\n",
    "            self.X = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.event = event\n",
    "        self.time = time\n",
    "        self.kernel = kernel # only support squared euclidean distance\n",
    "        self.bw = bw\n",
    "\n",
    "    def predict(self, time_test, X_test):\n",
    "        dist_mat_test_to_train = pairwise_distances(self.scaler.transform(X_test), self.X, metric='sqeuclidean')\n",
    "        B_mat = self.kernel(dist_mat_test_to_train/self.bw)\n",
    "        B_mat += 10e-10\n",
    "        indicator_mat = self.time.reshape(-1, 1) >= self.time.reshape(1, -1)\n",
    "        # eta = (self.time.reshape(-1, 1) <= t.reshape(1, -1))*self.event.reshape(-1, 1)\n",
    "        eta = (self.time.reshape(1, -1) <= time_test.reshape(-1, 1))*self.event.reshape(1, -1)\n",
    "        product_term_mat = (1 - B_mat/np.matmul(B_mat, indicator_mat))**eta\n",
    "        F_hat = 1 - np.product(product_term_mat, axis=1)\n",
    "        return F_hat\n",
    "    \n",
    "def augment_data(X, time, event, tau, F_hat):\n",
    "    \"\"\" Augment the data for Redistribution of Mass (RM) method of CQR\"\"\"\n",
    "    augment_index = np.where(((1 - event)*(F_hat < tau)))[0]\n",
    "    weight_train = np.ones(len(event))\n",
    "    weight_train[augment_index] = ((tau - F_hat)/(1 - F_hat + 10e-10))[augment_index]\n",
    "    weight_train = np.concatenate([weight_train, 1 - weight_train[augment_index]])\n",
    "\n",
    "    n_augment = len(augment_index)\n",
    "    augment_index = np.concatenate([np.arange(len(event)), augment_index])\n",
    "    X_augmented = X[augment_index]\n",
    "    event_augmented = event[augment_index]\n",
    "    time_augmented = np.concatenate([time, np.repeat(time.max()*100, n_augment)])\n",
    "    return X_augmented, time_augmented, event_augmented, weight_train\n",
    "\n",
    "class LocallyReweightedCQR:\n",
    "    def __init__(self):\n",
    "        self.X_train = None\n",
    "        self.event_train = None\n",
    "        self.time_train = None\n",
    "        self.beta = None\n",
    "        \n",
    "    def compute_weight(self, X_train, event_train, time_train, tau, kernel=lambda x: np.exp(-x), bw=0.01, is_minmax_normalize=True):\n",
    "        self.local_km_model = Local_KM(X_train, event_train, time_train, kernel, bw, is_minmax_normalize)\n",
    "        F_hat = self.local_km_model.predict(time_train, X_train)\n",
    "        self.weight_train = np.ones(len(event_train))\n",
    "        augment_index = np.where(((1 - event_train)*(F_hat < tau)))[0]\n",
    "        self.weight_train[augment_index] = ((tau - F_hat)/(1 - F_hat + 10e-10))[augment_index]\n",
    "        self.weight_train = self.weight_train.reshape((-1,1))\n",
    "        self.weight_train = np.concatenate([self.weight_train, 1 - self.weight_train], axis = 1)\n",
    "        # sim_X_augmented, sim_time_augmented, sim_event_augmented, weight_train = augment_data(X_train, time_train, event_train, tau, F_hat)\n",
    "\n",
    "    def quantile_loss(self, y_true, y_pred, tau):\n",
    "        \"\"\" Quantile loss function\"\"\"\n",
    "        return np.maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))\n",
    "        \n",
    "    def censored_quantile_loss(self, beta, X_train, time_train, tau, weight_train):\n",
    "        quantile_pred = np.dot(X_train, beta)\n",
    "        return np.dot(self.quantile_loss(time_train, quantile_pred, tau), weight_train[:,0]) + np.dot(self.quantile_loss(100*np.max(time_train), quantile_pred, tau), weight_train[:,1])\n",
    "    \n",
    "    def fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faaca37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, tau):\n",
    "    \"\"\" Quantile loss function for CQR \"\"\"\n",
    "    return np.mean(np.maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b3c9d0",
   "metadata": {},
   "source": [
    "## CQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245e27fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: [-0.00239328  0.05372781], RMSE: [0.22098931 0.37796986], Censor Rate: 0.39849999999999997\n"
     ]
    }
   ],
   "source": [
    "sim_n = 200\n",
    "sim_tau = 0.5\n",
    "sim_B = 100  # Number of bootstrap samples\n",
    "sim_cqr_beta = np.zeros((sim_B, 2))  # Placeholder for CQR beta coefficients\n",
    "censor_rate = 0\n",
    "set_random_seed()\n",
    "for i in range(sim_B):\n",
    "    sim_X, sim_failure_time, sim_censor_time, sim_event, sim_time = simulate_dgp_1(sim_n=sim_n, sim_tau=sim_tau, M = 14)\n",
    "    censor_rate += sim_event.mean()\n",
    "    local_km_model = Local_KM(sim_X, sim_event, sim_time, lambda x: np.exp(-x), 0.01)\n",
    "    F_hat = local_km_model.predict(sim_time, sim_X)\n",
    "    sim_X_augmented, sim_time_augmented, sim_event_augmented, weight_train = augment_data(sim_X, sim_time, sim_event, sim_tau, F_hat)\n",
    "    cqr_model = QuantileRegressor(quantile=sim_tau, alpha=0.0, fit_intercept=True, solver='highs')\n",
    "    cqr_model.fit(sim_X_augmented, sim_time_augmented, sample_weight=weight_train)\n",
    "    sim_cqr_beta[i, 0] = cqr_model.intercept_\n",
    "    sim_cqr_beta[i, 1] = cqr_model.coef_\n",
    "\n",
    "censor_rate /= sim_B\n",
    "censor_rate = 1 - censor_rate\n",
    "cqr_beta = np.array([3, 5])\n",
    "diff = sim_cqr_beta - cqr_beta\n",
    "bias = diff.mean(axis=0)\n",
    "rmse = diff.std(axis=0)\n",
    "print(f\"Bias: {bias}, RMSE: {rmse}, Censor Rate: {censor_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c846200",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocallyReweightedCQR()\n",
    "model.compute_weight(sim_X, sim_event, sim_time, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c332084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.4583798 , 0.5416202 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.47364431, 0.52635569],\n",
       "       [0.3955899 , 0.6044101 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49998747, 0.50001253],\n",
       "       [0.49999999, 0.50000001],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49961227, 0.50038773],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.4596086 , 0.5403914 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0446052 , 0.9553948 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49999495, 0.50000505],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.38700838, 0.61299162],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.49999682, 0.50000318],\n",
       "       [0.43745556, 0.56254444],\n",
       "       [0.49361919, 0.50638081],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49977272, 0.50022728],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49999997, 0.50000003],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49245014, 0.50754986],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.40632069, 0.59367931],\n",
       "       [0.49999912, 0.50000088],\n",
       "       [0.45906953, 0.54093047],\n",
       "       [0.48159692, 0.51840308],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24271791, 0.75728209],\n",
       "       [1.        , 0.        ],\n",
       "       [0.39602913, 0.60397087],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.00783182, 0.99216818],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49990909, 0.50009091],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.31775869, 0.68224131],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49945092, 0.50054908],\n",
       "       [0.45317642, 0.54682358],\n",
       "       [0.49999747, 0.50000253],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.42456636, 0.57543364],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.49652031, 0.50347969],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.23749325, 0.76250675],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49987262, 0.50012738],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.41570789, 0.58429211],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49726726, 0.50273274],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.49997674, 0.50002326],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d6859",
   "metadata": {},
   "source": [
    "## DRO simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e8419a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_cqr_dgp(sim_beta, sim_n=200, sim_tau=0.5, n_cov=1, cov_dist=[\"uniform\", 0.0, 1.0], censor_dist=[\"uniform\", 0.0, 14.0]):\n",
    "    def simulate_dist(dist_params, size):\n",
    "        if dist_params[0] == \"uniform\":\n",
    "            return uniform(low=dist_params[1], high=dist_params[2], size=size)\n",
    "        elif dist_params[0] == \"normal\":\n",
    "            return normal(loc=dist_params[1], scale=dist_params[2], size=size)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distribution type\")\n",
    "    sim_X = simulate_dist(cov_dist, (sim_n, n_cov))\n",
    "    sim_eps = normal(loc=0.0, scale=1.0, size=sim_n) - norm.ppf(sim_tau) # error term with tau-th quantile = 0\n",
    "    sim_failure_time = sim_beta[0] + np.dot(sim_X, sim_beta[1:]) + sim_eps\n",
    "    sim_censor_time = simulate_dist(censor_dist, sim_n)\n",
    "    sim_event = (sim_failure_time <= sim_censor_time) # event indicator\n",
    "    sim_time = sim_failure_time*sim_event + sim_censor_time*(~sim_event) # observed time\n",
    "    return sim_X, sim_failure_time, sim_censor_time, sim_event, sim_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1d9ce8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_X, sim_failure_time, sim_censor_time, sim_event, sim_time = simulate_cqr_dgp(sim_beta=cqr_beta, sim_n=200, sim_tau=0.5, n_cov=1, cov_dist=[\"normal\", 0.0, 1.0], censor_dist=[\"uniform\", 0.0, 14.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2dbba9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed()\n",
    "# Training set parameters\n",
    "sim_n_train = 500 # Number of training samples in one batch\n",
    "sim_tau = 0.95 # Quantile to estimate\n",
    "sim_B = 500  # Number of bootstrap samples\n",
    "n_cov = 5 # Number of covariates\n",
    "\n",
    "# Testing set parameters (distributional shift)\n",
    "sim_n_test = 10000 # Number of testing samples in one batch\n",
    "shift_loc = 0.5 # Location for beta coefficients shift\n",
    "shift_scale = 0.1 # Shift scale for beta coefficients\n",
    "\n",
    "# Simulation\n",
    "sim_beta = normal(loc=0.0, scale=1.0, size=n_cov + 1)  # Simulated beta coefficients\n",
    "sim_beta_shifted = sim_beta + normal(loc=shift_loc, scale=shift_scale, size=n_cov + 1)  # Shifted beta coefficients\n",
    "sim_cqr_beta = np.zeros((sim_B, n_cov + 1))  # Placeholder for CQR beta coefficients\n",
    "sim_loss_test = np.zeros(sim_B)  # Placeholder for test loss\n",
    "# censor_rate = 0\n",
    "censor_mean = sim_beta[0] - norm.ppf(sim_tau) - norm.ppf(0.4)*np.sqrt(np.dot(sim_beta[1:], sim_beta[1:]) + 2)\n",
    "for i in range(sim_B):\n",
    "    # Training set generation\n",
    "    sim_X, sim_failure_time, sim_censor_time, sim_event, sim_time = simulate_cqr_dgp(sim_beta=sim_beta, \n",
    "                                                                                     sim_n=sim_n_train, sim_tau=sim_tau, n_cov=n_cov, \n",
    "                                                                                     cov_dist=[\"normal\", 0.0, 1.0], \n",
    "                                                                                     censor_dist=[\"normal\", censor_mean, 1.0])\n",
    "    # censor_rate += sim_event.mean()\n",
    "    local_km_model = Local_KM(sim_X, sim_event, sim_time, lambda x: np.exp(-x), 0.01)\n",
    "    F_hat = local_km_model.predict(sim_time, sim_X)\n",
    "    sim_X_augmented, sim_time_augmented, sim_event_augmented, weight_train = augment_data(sim_X, sim_time, sim_event, sim_tau, F_hat)\n",
    "    cqr_model = QuantileRegressor(quantile=sim_tau, alpha=0.0, fit_intercept=True, solver='highs')\n",
    "    cqr_model.fit(sim_X_augmented, sim_time_augmented, sample_weight=weight_train)\n",
    "    sim_cqr_beta[i, 0] = cqr_model.intercept_\n",
    "    sim_cqr_beta[i, 1:] = cqr_model.coef_\n",
    "\n",
    "    # Testing set generation\n",
    "    sim_X_test, sim_failure_time_test, sim_censor_time_test, sim_event_test, sim_time_test = simulate_cqr_dgp(sim_beta=sim_beta_shifted, \n",
    "                                                                                                                sim_n=sim_n_test, sim_tau=sim_tau, n_cov=n_cov, \n",
    "                                                                                                                cov_dist=[\"normal\", 0.0, 1.0], \n",
    "                                                                                                                censor_dist=[\"normal\", censor_mean, 1.0])\n",
    "    \n",
    "    quantile_pred = cqr_model.predict(sim_X_test)\n",
    "    # sim_loss_test[i] = quantile_loss(sim_time_test, quantile_pred, sim_tau)\n",
    "    sim_loss_test[i] = np.mean((sim_beta_shifted[0] + np.dot(sim_X_test, sim_beta_shifted[1:]) - quantile_pred)**2)  # Using mean absolute error as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e53296e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: [-0.35340284  0.05708369  0.05491011  0.09537443 -0.0829458   0.21946099], RMSE: [0.14691833 0.11614278 0.11968004 0.12798962 0.11967814 0.1639043 ]\n",
      "Censor Rate: 0.388\n"
     ]
    }
   ],
   "source": [
    "diff = sim_cqr_beta - sim_beta\n",
    "bias = diff.mean(axis=0)\n",
    "rmse = diff.std(axis=0)\n",
    "print(f\"Bias: {bias}, RMSE: {rmse}\\nCensor Rate: {1 - sim_event.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ebe2d99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763,\n",
       "       -2.3015387 ])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a9386442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4859305283679392"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_loss_test.mean()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80bc1f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  4.,  7., 22., 50., 48., 36., 22.,  9.]),\n",
       " array([-17.98743186, -14.7308317 , -11.47423154,  -8.21763138,\n",
       "         -4.96103123,  -1.70443107,   1.55216909,   4.80876925,\n",
       "          8.06536941,  11.32196956,  14.57856972]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(sim_failure_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fbaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10039333],\n",
       "       [0.97275282],\n",
       "       [0.0095748 ],\n",
       "       [0.45543118],\n",
       "       [0.81085683],\n",
       "       [0.78001388],\n",
       "       [0.06413688],\n",
       "       [0.87471135],\n",
       "       [0.85630963],\n",
       "       [0.53135207],\n",
       "       [0.37807862],\n",
       "       [0.49465001],\n",
       "       [0.86172838],\n",
       "       [0.88137806],\n",
       "       [0.97683484],\n",
       "       [0.69498226],\n",
       "       [0.66053796],\n",
       "       [0.22890842],\n",
       "       [0.147044  ],\n",
       "       [0.77961088],\n",
       "       [0.90389541],\n",
       "       [0.3883962 ],\n",
       "       [0.16548612],\n",
       "       [0.43338058],\n",
       "       [0.49883089],\n",
       "       [0.99964079],\n",
       "       [0.26723552],\n",
       "       [0.04488118],\n",
       "       [0.22978819],\n",
       "       [0.26740881],\n",
       "       [0.41229027],\n",
       "       [0.71089572],\n",
       "       [0.94368305],\n",
       "       [0.34399921],\n",
       "       [0.75735672],\n",
       "       [0.08213307],\n",
       "       [0.74110338],\n",
       "       [0.9398945 ],\n",
       "       [0.59422156],\n",
       "       [0.1852225 ],\n",
       "       [0.59632923],\n",
       "       [0.8395928 ],\n",
       "       [0.7299749 ],\n",
       "       [0.50267069],\n",
       "       [0.98888542],\n",
       "       [0.34969887],\n",
       "       [0.85104896],\n",
       "       [0.70626174],\n",
       "       [0.64915998],\n",
       "       [0.52660201],\n",
       "       [0.15551727],\n",
       "       [0.20419779],\n",
       "       [0.72476173],\n",
       "       [0.22450678],\n",
       "       [0.5862743 ],\n",
       "       [0.71549418],\n",
       "       [0.24411438],\n",
       "       [0.27433693],\n",
       "       [0.37172652],\n",
       "       [0.13663312],\n",
       "       [0.22919   ],\n",
       "       [0.97365594],\n",
       "       [0.90659187],\n",
       "       [0.11391209],\n",
       "       [0.37439411],\n",
       "       [0.58918896],\n",
       "       [0.3515674 ],\n",
       "       [0.81409636],\n",
       "       [0.00144895],\n",
       "       [0.7382084 ],\n",
       "       [0.46972767],\n",
       "       [0.21972205],\n",
       "       [0.81927045],\n",
       "       [0.05865086],\n",
       "       [0.67259286],\n",
       "       [0.85264391],\n",
       "       [0.88207063],\n",
       "       [0.1484385 ],\n",
       "       [0.1963758 ],\n",
       "       [0.97431708],\n",
       "       [0.1530333 ],\n",
       "       [0.96526047],\n",
       "       [0.78614193],\n",
       "       [0.84891251],\n",
       "       [0.68236498],\n",
       "       [0.21449192],\n",
       "       [0.51931531],\n",
       "       [0.99662398],\n",
       "       [0.62273125],\n",
       "       [0.41495991],\n",
       "       [0.71567858],\n",
       "       [0.73086737],\n",
       "       [0.10485236],\n",
       "       [0.74881213],\n",
       "       [0.70819653],\n",
       "       [0.37515935],\n",
       "       [0.03113687],\n",
       "       [0.58464247],\n",
       "       [0.4108754 ],\n",
       "       [0.25924594],\n",
       "       [0.53054218],\n",
       "       [0.40401573],\n",
       "       [0.95487654],\n",
       "       [0.89921742],\n",
       "       [0.47467493],\n",
       "       [0.11007413],\n",
       "       [0.96475365],\n",
       "       [0.1278436 ],\n",
       "       [0.32277906],\n",
       "       [0.11570869],\n",
       "       [0.20018328],\n",
       "       [0.34705256],\n",
       "       [0.7237171 ],\n",
       "       [0.67542354],\n",
       "       [0.9248966 ],\n",
       "       [0.04354387],\n",
       "       [0.50857392],\n",
       "       [0.49591895],\n",
       "       [0.46593448],\n",
       "       [0.98029964],\n",
       "       [0.78626212],\n",
       "       [0.75194869],\n",
       "       [0.51292257],\n",
       "       [0.9289709 ],\n",
       "       [0.76448206],\n",
       "       [0.14007179],\n",
       "       [0.27905524],\n",
       "       [0.26669194],\n",
       "       [0.19693556],\n",
       "       [0.5629035 ],\n",
       "       [0.93464171],\n",
       "       [0.02175727],\n",
       "       [0.95052286],\n",
       "       [0.58236746],\n",
       "       [0.47163239],\n",
       "       [0.22401525],\n",
       "       [0.04118843],\n",
       "       [0.13754107],\n",
       "       [0.76937647],\n",
       "       [0.42242273],\n",
       "       [0.61027963],\n",
       "       [0.79343211],\n",
       "       [0.39673833],\n",
       "       [0.77503918],\n",
       "       [0.16079236],\n",
       "       [0.12742804],\n",
       "       [0.87500748],\n",
       "       [0.86079051],\n",
       "       [0.70133215],\n",
       "       [0.68954629],\n",
       "       [0.41437572],\n",
       "       [0.83500121],\n",
       "       [0.34309931],\n",
       "       [0.11689443],\n",
       "       [0.07224626],\n",
       "       [0.50272806],\n",
       "       [0.76387425],\n",
       "       [0.28697477],\n",
       "       [0.3113374 ],\n",
       "       [0.32052675],\n",
       "       [0.23572795],\n",
       "       [0.77184337],\n",
       "       [0.28834612],\n",
       "       [0.26755298],\n",
       "       [0.95420475],\n",
       "       [0.2178295 ],\n",
       "       [0.11939936],\n",
       "       [0.84148088],\n",
       "       [0.67246008],\n",
       "       [0.41250373],\n",
       "       [0.59853404],\n",
       "       [0.05161734],\n",
       "       [0.17931231],\n",
       "       [0.3592997 ],\n",
       "       [0.42085951],\n",
       "       [0.09337702],\n",
       "       [0.00777528],\n",
       "       [0.6513529 ],\n",
       "       [0.95851533],\n",
       "       [0.36466397],\n",
       "       [0.40498207],\n",
       "       [0.63749326],\n",
       "       [0.20821635],\n",
       "       [0.26981809],\n",
       "       [0.18626203],\n",
       "       [0.05398422],\n",
       "       [0.02520417],\n",
       "       [0.23857197],\n",
       "       [0.09035036],\n",
       "       [0.82129417],\n",
       "       [0.63533232],\n",
       "       [0.44513465],\n",
       "       [0.38225556],\n",
       "       [0.9029599 ],\n",
       "       [0.40965787],\n",
       "       [0.57485525],\n",
       "       [0.76618438],\n",
       "       [0.25471115],\n",
       "       [0.93297406],\n",
       "       [0.50375767]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(sim_X, cqr_beta[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dro_surv_py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
